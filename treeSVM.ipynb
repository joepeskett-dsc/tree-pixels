{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an SVM to veg-colour data\n",
    "\n",
    "1. split into train, val and test sets\n",
    "2. scale features\n",
    "3. Try SVM with linear Kernel first - find optimal value for C to minimize error of validation set\n",
    "4. Try to add new features or use Gaussian Kernel\n",
    "\n",
    "** only 1000 data points are used to begin with to speed up processing time while we are getting the pipeline working**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.csv\", header= None)\n",
    "#data = data.iloc[1:100000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:2]\n",
    "Y = data.iloc[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, val and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, train_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training, validation and test sets, we need to scale our features. For this I have used the preprocessing tool found in scikitlearn. This stores the properties used to scale the data so that it can be used later on the validation and test sets. For speed we have scaled these two sets now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_val =scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll implement a for loop to find an optimum value for C in the SVM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = [30, 10, 0.01,0.03, 0.1, 0.3, 1, 3, 0.0001]\n",
    "min_error = 100000000000000000000000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll try a test SVM on the 1000 row test set before attempting to optimise SVM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C = 1, kernel = 'linear')\n",
    "clf.fit(X_train, y_train)\n",
    "error = clf.score(X_val, y_val)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C = 1, kernel = 'rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "error = clf.score(X_val, y_val)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we try to optimise the SVM parameters for 1,000 row test set. We will use the rbf kernel set to default to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "error_min = 100\n",
    "list_C = list()\n",
    "list_error = list()\n",
    "for i in values:\n",
    "    clf = svm.SVC(C = i, kernel = 'rbf')\n",
    "    clf.fit(X_train, y_train)\n",
    "    error = 1 - clf.score(X_val, y_val)\n",
    "    list_C.append(i)\n",
    "    list_error.append(error)\n",
    "    if error < error_min:\n",
    "        error_min = error\n",
    "        optim_C = i\n",
    "        optim_model = clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_error)\n",
    "print(list_C)\n",
    "print(optim_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The SVM is currently taking a long time to run, so we will now try using logistic regression, which should lead to similar results in this problem. We will use the same train, validation and test sets as above which have already been scaled. We will also add polynomial features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression()\n",
    "logreg.fit(X_test, y_test)\n",
    "error = logreg.score(X_val, y_val)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = [30, 10, 0.01,0.03, 0.1, 0.3, 1, 3, 0.0001]\n",
    "error_min = 100\n",
    "list_C = list()\n",
    "list_error = list()\n",
    "for i in values:\n",
    "    logreg = sklearn.linear_model.LogisticRegression(C = i)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    error = 1 - logreg.score(X_val, y_val)\n",
    "    list_C.append(i)\n",
    "    list_error.append(error)\n",
    "    if error < error_min:\n",
    "        error_min = error\n",
    "        optim_C = i\n",
    "        optim_model = logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_error)\n",
    "print(list_C)\n",
    "print(optim_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With logistic regression we have nearly exacrtly the same level of accuracy ~68%. To try and improve on this we'll try adding some new features. We'll start completely from scratch to do this. \n",
    "\n",
    "## Restart the notebook here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"training.csv\", header= None)\n",
    "#data = data.iloc[1:100000,:]\n",
    "X = data.iloc[:,0:2]\n",
    "Y = data.iloc[:,2]\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scale = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have scaled all our X values - we will know add additional polynomial features. This can be done easily using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300548, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = sklearn.preprocessing.PolynomialFeatures(degree = 5)\n",
    "X_poly = poly.fit_transform(X_scale)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 28 features in total, increasing the complexity of our model. Next we split the data into train, validation and test sets in the same 60:20:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, Y, test_size = 0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, train_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing 0.01\n",
      "testing 0.3\n",
      "testing 1\n",
      "testing 3\n",
      "testing 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "values = [0.01, 0.3, 1, 3, 100]\n",
    "error_min = 100\n",
    "list_C = list()\n",
    "list_error = list()\n",
    "for i in values:\n",
    "    print('testing', i)\n",
    "    logreg = sklearn.linear_model.LogisticRegression(C = i)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    error = 1 - logreg.score(X_val, y_val)\n",
    "    list_C.append(i)\n",
    "    list_error.append(error)\n",
    "    if error < error_min:\n",
    "        error_min = error\n",
    "        optim_C = i\n",
    "        optim_model = logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32402409313472635, 0.32379316091452337, 0.32339160866908445, 0.32537953035846079, 0.32128068184523573]\n",
      "[0.01, 0.3, 1, 3, 100]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(list_error)\n",
    "print(list_C)\n",
    "print(optim_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67805032893953698"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
