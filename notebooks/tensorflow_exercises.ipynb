{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeDq/4/vP4kXwhdvWlaUxw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joepeskett/tree-pixels/blob/master/notebooks/tensorflow_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmL6-D66BAPz",
        "colab_type": "text"
      },
      "source": [
        "# Exercises to experiment with Tensorflow2/Keras\n",
        "\n",
        "This is a notebook to go through some of the exercises from Hands on Machine Learning v2. \n",
        "\n",
        "1. Implement a custom layer for layer normalisation\n",
        "\n",
        "2. Train a model using a custom training loop\n",
        "\n",
        "3. Use the `tf.data.Dataset` functionality for preprocessing and then create a basic binary classifier. \n",
        "\n",
        "That should do for now. First, let's check that we're using Tensorflow 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRcDOMM_A8Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGdWWgTqWfIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTjgRl3cA-ZS",
        "colab_type": "text"
      },
      "source": [
        "# Implementing a custom layer for Layer Normalisation\n",
        "\n",
        "Firstly, what is layer normalisation(LN). Rather than normalising across the batch, as seen in Batch Normalisation(BN), LN normalises across the features dimension. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQOPF-snEDPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras as keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACdL5wQAENG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNormalisation(keras.layers.Layer):\n",
        "  # Test that this layer gives the same output as the keras.layers.LayerNorm\n",
        "  # layer. \n",
        "  def __init__(self, eps = 0.001, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.eps = eps\n",
        "  def build(self, batch_input_shape):\n",
        "    # This method should define two trainable weights \n",
        "    # This method needs to call the add_weight function\n",
        "    # Essentially, this method is \n",
        "    self.alpha = self.add_weight(name = \"alpha\", \n",
        "                                 shape = batch_input_shape[-1:], \n",
        "                                 initializer = \"ones\")\n",
        "    self.beta = self.add_weight(name = \"beta\",\n",
        "                                shape = batch_input_shape[-1:],\n",
        "                                initializer = \"zeros\")\n",
        "    # We need to call the super's build method AT THE END(!)\n",
        "    super().build(batch_input_shape)\n",
        "  def call(self, X):\n",
        "    # This method should compute the mean and std dev of the each instances \n",
        "    # features. You can use tf.nn.moments()\n",
        "    # This function can then be used to calculate the \n",
        "    mean, variance = tf.nn.moments(X, axes = 1, keepdims=True)\n",
        "    return self.alpha * (X - mean)/(tf.sqrt(variance+self.eps)) + self.beta\n",
        "\n",
        "  # We need to more methods to complete a layer: `compute_output_shape` and `get_config`\n",
        "  \n",
        "  # Because we're not adding to the shape in this layer, we just return the\n",
        "  # same shape as the batch_input_shape\n",
        "  def get_output_shape(self, batch_input_shape):\n",
        "    return batch_input_shape\n",
        "\n",
        "  # Any additional configs that we have added to the base class\n",
        "  def get_config(self):\n",
        "    base_config = super().get_config\n",
        "    return {**base_config, \"eps1:\" : self.eps}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27BswUZ4ioVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing our new layer \n",
        "X = X_train.astype(np.float32)\n",
        "\n",
        "custom_layer_norm = LayerNormalisation()\n",
        "keras_layer_norm = keras.layers.LayerNormalization()\n",
        "\n",
        "tf.reduce_mean(keras.losses.mean_absolute_error(\n",
        "    keras_layer_norm(X), custom_layer_norm(X)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEOGHYdTGRUw",
        "colab_type": "text"
      },
      "source": [
        "# Train a model using a custom training loop\n",
        "\n",
        "Generally, we should not *need* to do this very often, given the extensive functionality now provided by Tensorflow. \n",
        "\n",
        "A custom training loop can be useful when we want to use different optmisers for different parts of the network. The book refers to the wide and deep architecture. \n",
        "\n",
        "First, let's look at an example from the book, and then move on to our own example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5C7RN5ZFS5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_reg = keras.regularizers.l2(0.05)\n",
        "#Define our model\n",
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.Dense(30, activation = \"elu\",\n",
        "                                                    kernel_initializer = \"he_normal\",\n",
        "                                                    kernel_regularizer = l2_reg),\n",
        "                                 keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
        "])\n",
        "\n",
        "def random_batch(X, y, batch_size = 32):\n",
        "  #Create a random batch for each step\n",
        "  idx = np.random.randint(len(X), size = batch_size)\n",
        "  return X[idx], y[idx]\n",
        "\n",
        "def print_status_bar(iteration, total, loss, metrics=None):\n",
        "  # Define a function for the status bar that is printed after each step.\n",
        "  metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) \n",
        "                      for m in [loss] + (metrics or [])])\n",
        "  end = \" \" if iteration < total else \"\\n\"\n",
        "  print(\"\\r{}/{} - \".format(iteration, total)+metrics, end = end)\n",
        "\n",
        "\n",
        "# Set up some basic variables\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "n_steps = len(X_train) // batch_size\n",
        "optimizer = keras.optimizers.Nadam(lr = 0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "mean_loss = keras.metrics.Mean()\n",
        "metrics = [keras.metrics.MeanAbsoluteError()]\n",
        "\n",
        "# And now we setup the basic training loop...\n",
        "\n",
        "#Create a loop for each epoch\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
        "  #Create a loop for each step within the epoch\n",
        "  for step in range(1, n_steps +1):\n",
        "    #Sample a random batch\n",
        "    X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
        "    #Use Gradient Tape\n",
        "    with tf.GradientTape() as tape:\n",
        "      #Make a prediction using the model function\n",
        "      y_pred = model(X_batch, training = True)\n",
        "      #Compute the loss\n",
        "      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "      # Include the regularisation losses \n",
        "      loss = tf.add_n([main_loss], + model.loss)\n",
        "    # Then we ask the tape to calculate the gradient of the loss in regards to \n",
        "    # to each trainable variable.\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #Apply these gradients to the optimizer to perform gradient descent.\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    #-->Weight contraints should be included here <--\n",
        "    \n",
        "    #Update the metrics\n",
        "    mean_loss(loss)\n",
        "    for metric in metrics:\n",
        "      metric(y_batch, y_pred)\n",
        "    #Display the metrics in the status bar.\n",
        "    print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
        "    for metric in [mean_loss] + metrics:\n",
        "      metric.reset_states()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE5WwRAJR79y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create our own training loop\n",
        "# We want to display the epoch, iteration, mean training loss and  mean accuracy over each epoch updated at each iteration\n",
        "# We want the validation loss and accuracy at the end of each epoch\n",
        "# Finally, try and use different optimisers with different learning rates for the upper layers and lower layers.\n",
        "\n",
        "# Data setup - we create a validation data set for the validation loss and accuracy metrics\n",
        "\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full.astype(np.float32) / 255.\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test.astype(np.float32) / 255.\n",
        "print(X_train.shape)\n",
        "l2_reg = keras.regularizers.l2(0.05)\n",
        "#Define our model\n",
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.Flatten(),\n",
        "                                 keras.layers.Dense(100, activation = \"relu\",\n",
        "                                                    kernel_initializer = \"he_normal\",\n",
        "                                                    kernel_regularizer = l2_reg),\n",
        "                                 keras.layers.Dense(10, \n",
        "                                                    activation = \"softmax\", \n",
        "                                                    kernel_regularizer=l2_reg)\n",
        "])\n",
        "\n",
        "def random_batch(X, y, batch_size = 32):\n",
        "  #Create a random batch for each step\n",
        "  idx = np.random.randint(len(X), size = batch_size)\n",
        "  return X[idx], y[idx]\n",
        "\n",
        "def print_status_bar(iteration, total, loss, metrics=None):\n",
        "  # Define a function for the status bar that is printed after each step.\n",
        "  metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) \n",
        "                      for m in [loss] + (metrics or [])])\n",
        "  end = \" \" if iteration < total else \"\\n\"\n",
        "  print(\"\\r{}/{} - \".format(iteration, total)+metrics, end = end)\n",
        "\n",
        "\n",
        "# Set up some basic variables\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "n_steps = len(X_train) // batch_size\n",
        "optimizer = keras.optimizers.Nadam(lr = 0.01)\n",
        "optimizer_low = keras.optimizers.Adagrad(learning_rate = 0.001)\n",
        "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
        "mean_loss = keras.metrics.Mean()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "# We'll create a function to output the required validation metrics\n",
        "\n",
        "def print_validation_metrics(X_val, y_val):\n",
        "  #Run the model over the validation dataset\n",
        "  y_pred = model(X_val, training = False)\n",
        "  val_loss = np.mean(loss_fn(y_val, y_pred))\n",
        "  val_acc = np.mean(keras.metrics.sparse_categorical_accuracy(\n",
        "      tf.constant(y_val, dtype = np.float32),y_pred))\n",
        "  print(\"Validation metrics: loss = {}; accuracy = {}\".format(val_loss, val_acc))\n",
        "\n",
        "# And now we setup the basic training loop...\n",
        "\n",
        "#Create a loop for each epoch\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
        "  #Create a loop for each step within the epoch\n",
        "  for step in range(1, n_steps +1):\n",
        "    #Sample a random batch\n",
        "    X_batch, y_batch = random_batch(X_train, y_train)\n",
        "    #Use Gradient Tape\n",
        "    with tf.GradientTape() as tape:\n",
        "      #Make a prediction using the model function\n",
        "      y_pred = model(X_batch, training = True)\n",
        "      #Compute the loss\n",
        "      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "      # Include the regularisation losses \n",
        "      loss = tf.add_n([main_loss] + model.losses)\n",
        "    # Then we ask the tape to calculate the gradient of the loss in regards to \n",
        "    # to each trainable variable.\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #Apply these gradients to the optimizer to perform gradient descent.\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    #-->Weight contraints should be included here <--\n",
        "    \n",
        "    #Update the metrics\n",
        "    mean_loss(loss)\n",
        "    for metric in metrics:\n",
        "      metric(y_batch, y_pred)\n",
        "    #Display the metrics in the status bar.\n",
        "    print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
        "    for metric in [mean_loss] + metrics:\n",
        "      metric.reset_states()\n",
        "  \n",
        "  # Here we're going to add in print step for the valdation loss and accuracy\n",
        "  print_validation_metrics(X_valid, y_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GH5gOFGf32",
        "colab_type": "text"
      },
      "source": [
        "# Optimisers for different levels of the network\n",
        "\n",
        "Now we want to look at how we change the type of optimiser at different points in the network similar to what we saw in the wide and deep paper. \n",
        "\n",
        "This got a little more complicated for me so I had a look through the solution. Using the sequential API we can create our network in two pieces. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa1XsCC5VBjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}