{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-Datasets",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKHFQjc4k3zmYUv+UaBTEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joepeskett/tree-pixels/blob/master/notebooks/TF_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x0vf4NOIxcp",
        "colab_type": "text"
      },
      "source": [
        "# Using TF Datasets\n",
        "\n",
        "There are some really good walkthroughs of how this can be achieved in the [tensorflow data api documentation](https://www.tensorflow.org/guide/data#consuming_text_data). \n",
        "\n",
        "Get the dataset from stanford"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMfwk10ItlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xvf aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzJ7Mak-I3IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "#dataset = tfds.load(\"imdb_reviews\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrTJM3uI3F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to create a similar dataset to the one found in the the tfds package. \n",
        "# How we would we efficiently load these files to create a useful dataset\n",
        "\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLN9IUkWYtaL",
        "colab_type": "text"
      },
      "source": [
        "Notes about this data set - the rating for each movie is in the filename, as is the unique ID of the particular review. \n",
        "\n",
        "A good first step would be to have a setup to have a unique ID, text and the rating. \n",
        "\n",
        "TODO:\n",
        "\n",
        "* List the files within the positive and negative training directories. \n",
        "* Load these into an appropriate training data set\n",
        "* Ensure each piece is text is labelled correctly\n",
        "* Shuffle these dataset\n",
        "* Create an appropriate representation for the text data, likely using the TextVectorisation layer within Keras\n",
        "* Add an embedding layer\n",
        "* Train a model \n",
        "* Try to optimise the pipeline that we've prepared"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "besmt-PCI3Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "DIRS = [\"aclImdb/train/pos/\", \"aclImdb/train/neg/\"]\n",
        "files = []\n",
        "for d in DIRS:\n",
        "  for f in os.listdir(d):\n",
        "    files.append(os.path.join(d, f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytA7zlnnI28_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple function to grab the ID and rating from the file name\n",
        "# We want to return tf.constants\n",
        "def labeller(file_name):\n",
        "  id, rating = os.path.splitext(os.path.basename(file_name))[0].split(\"_\")\n",
        "  return [tf.constant(id), tf.constant(rating)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_85Ce4si4LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create tf \n",
        "labeller(files[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5BCWZ_ooqfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path_dataset = tf.data.Dataset.list_files(files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0-Fu75Co1L3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9riud4ZI23C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we want to create a data set for each individual file\n",
        "\n",
        "IDs = []\n",
        "reviews = [] \n",
        "ratings = []\n",
        "for filename in files[:10]:\n",
        "  lines_dataset = tf.data.TextLineDataset(filenames=filename)\n",
        "  id, rating = labeller(filename)\n",
        "  IDs.append(id)\n",
        "  ratings.append(rating)\n",
        "  reviews.append(lines_dataset)\n",
        "\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(tf.constant(IDs), tf.constant(reviews), tf.constant(ratings))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0B8JhgAI2zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_dataset = tf.data.TextLineDataset(os.path.join(files[1][0], files[1][1]))\n",
        "example_dataset = example_dataset.map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOrALvjOh0hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(example_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeyS-HiKh15t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}