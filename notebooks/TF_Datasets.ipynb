{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-Datasets",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkTW3F/7waEOwoCoZJll/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joepeskett/tree-pixels/blob/master/notebooks/TF_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x0vf4NOIxcp",
        "colab_type": "text"
      },
      "source": [
        "# Using TF Datasets\n",
        "\n",
        "There are some really good walkthroughs of how this can be achieved in the [tensorflow data api documentation](https://www.tensorflow.org/guide/data#consuming_text_data). \n",
        "\n",
        "Get the dataset from stanford"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMfwk10ItlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xvf aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzJ7Mak-I3IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "#dataset = tfds.load(\"imdb_reviews\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrTJM3uI3F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to create a similar dataset to the one found in the the tfds package. \n",
        "# How we would we efficiently load these files to create a useful dataset\n",
        "\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLN9IUkWYtaL",
        "colab_type": "text"
      },
      "source": [
        "Notes about this data set - the rating for each movie is in the filename, as is the unique ID of the particular review. \n",
        "\n",
        "A good first step would be to have a setup to have a unique ID, text and the rating. \n",
        "\n",
        "TODO:\n",
        "\n",
        "* List the files within the positive and negative training directories. \n",
        "* Load these into an appropriate training data set\n",
        "* Ensure each piece is text is labelled correctly\n",
        "* Shuffle these dataset\n",
        "* Create an appropriate representation for the text data, likely using the TextVectorisation layer within Keras\n",
        "* Add an embedding layer\n",
        "* Train a model \n",
        "* Try to optimise the pipeline that we've prepared"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "besmt-PCI3Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "DIRS = [\"aclImdb/train/pos/\", \"aclImdb/train/neg/\"]\n",
        "files = []\n",
        "for d in DIRS:\n",
        "  for f in os.listdir(d):\n",
        "    files.append(os.path.join(d, f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytA7zlnnI28_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple function to grab the ID and rating from the file name\n",
        "# We want to return tf.constants\n",
        "def dataset_creator(file_name):\n",
        "  id, rating = os.path.splitext(os.path.basename(file_name))[0].split(\"_\")\n",
        "  review = open(file_name).read()\n",
        "  return {'id':int(id), \n",
        "          'rating':int(rating), \n",
        "          'review':review}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9riud4ZI23C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we want to create a data set for each individual file\n",
        "IDs = []\n",
        "reviews = [] \n",
        "ratings = []\n",
        "for filename in files:\n",
        "  obs = dataset_creator(filename)\n",
        "  IDs.append(obs['id'])\n",
        "  ratings.append(obs['rating'])\n",
        "  reviews.append(obs['review'])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((tf.constant(IDs), tf.constant(ratings), tf.constant(reviews)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmOmJys5vnFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0B8JhgAI2zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOrALvjOh0hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(example_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeyS-HiKh15t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}