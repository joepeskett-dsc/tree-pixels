{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-Datasets",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSPdIOSAV/XqxDqPts+aWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joepeskett/tree-pixels/blob/master/notebooks/TF_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x0vf4NOIxcp",
        "colab_type": "text"
      },
      "source": [
        "# Using TF Datasets\n",
        "\n",
        "There are some really good walkthroughs of how this can be achieved in the [tensorflow data api documentation](https://www.tensorflow.org/guide/data#consuming_text_data). \n",
        "\n",
        "Get the dataset from stanford"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMfwk10ItlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xvf aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzJ7Mak-I3IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "#dataset = tfds.load(\"imdb_reviews\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrTJM3uI3F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to create a similar dataset to the one found in the the tfds package. \n",
        "# How we would we efficiently load these files to create a useful dataset\n",
        "\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLN9IUkWYtaL",
        "colab_type": "text"
      },
      "source": [
        "Notes about this data set - the rating for each movie is in the filename, as is the unique ID of the particular review. \n",
        "\n",
        "A good first step would be to have a setup to have a unique ID, text and the rating. \n",
        "\n",
        "TODO:\n",
        "\n",
        "* List the files within the positive and negative training directories. DONE\n",
        "* Load these into an appropriate training data set. DONE\n",
        "* Ensure each piece is text is labelled correctly. DONE\n",
        "* Shuffle these dataset. DONE\n",
        "* Create an appropriate representation for the text data, likely using the TextVectorisation layer within Keras.DOING\n",
        "* Add an embedding layer.TODO\n",
        "* Train a model.TODO\n",
        "* Try to optimise the pipeline that we've prepared.TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hkbKNVtdxD-",
        "colab_type": "text"
      },
      "source": [
        "# First: reformat the dataset.\n",
        "\n",
        "This is so we can make use of the TextLinesDataset ability in the `tf.data` API. \n",
        "\n",
        "Rather than having the rating in the filename, we'll change the txt files to individual "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "besmt-PCI3Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "DIRS = [\"aclImdb/train/pos/\", \"aclImdb/train/neg/\"]\n",
        "def create_file_list(dirs = DIRS):\n",
        "  files = []\n",
        "  for d in dirs:\n",
        "    for f in os.listdir(d):\n",
        "      files.append(os.path.join(d, f))\n",
        "  return files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD-RiqtlfC25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the ratings\n",
        "ratings = []\n",
        "file_names = create_file_list(DIRS)\n",
        "for f in file_names:\n",
        "  id, rating = os.path.splitext(os.path.basename(f))[0].split(\"_\")\n",
        "  ratings.append(int(rating))\n",
        "import numpy as np\n",
        "rate_array = np.array(ratings)\n",
        "np.histogram(rate_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytA7zlnnI28_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple function to grab the ID and rating from the file name\n",
        "# We want to return tf.constants\n",
        "import pandas as pd\n",
        "def dataset_creator(file_name, dir_name = \"train\"):\n",
        "  id, rating = os.path.splitext(os.path.basename(file_name))[0].split(\"_\")\n",
        "  review = open(file_name).read()\n",
        "  output =  {'id':int(id), \n",
        "             'rating':int(rating), \n",
        "             'review':review}\n",
        "  output_file = pd.DataFrame(output, index = [0])\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)\n",
        "  output_file.to_csv('train/{}.csv'.format(id+\"_\"+rating))\n",
        "\n",
        "def create_files(dirs = DIRS, output_dir=\"train\"):\n",
        "  files = create_file_list(dirs)\n",
        "  for f in files:\n",
        "    dataset_creator(f, output_dir)\n",
        "  return \"DONE!\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMhJOVk-dSmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_files(DIRS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCvv3tXGTWTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffJ_qjQLTEg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(os.listdir(\"aclImdb/train/pos/\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9a0b6s9TzO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(os.listdir(\"aclImdb/train/neg/\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwWcyydUT2I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(os.listdir(\"train\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLnYD3zPgLIp",
        "colab_type": "text"
      },
      "source": [
        "Now, we can start looking at how we'd use the tensorflow api.\n",
        "The key reason to do this is to mimic a setup where we can't git all our training data into memory or we want to start training on a distributed system. \n",
        "\n",
        "In this setup, it's not that easy to use the filename in the initial dataset creation, which is why we've done some initial preprocessing in the above section. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9riud4ZI23C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Now we want to create a data set for each individual file\n",
        "file_path_dataset = tf.data.Dataset.list_files(\"train/*.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmOmJys5vnFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_readers = 5\n",
        "dataset = file_path_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length = n_readers\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lvOnMAzjLhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#What have we got so far?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eck8gpzDi-_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in dataset.take(5):\n",
        "  print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbJAL5cRjnRE",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing the raw data into Tensors\n",
        "\n",
        "At this point we need a preprocessing function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJH7JHMRlThT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_raw_input(line):\n",
        "  #defs = #The default values o\n",
        "  record_defaults = [[1],[1],[1],tf.constant([], dtype=tf.string)]\n",
        "  fields = tf.io.decode_csv(line, record_defaults=record_defaults)\n",
        "  X = tf.stack(fields[3])\n",
        "  y = tf.stack(fields[2])\n",
        "  return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVcftg2_mqsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in dataset.take(1):\n",
        "  print(process_raw_input(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7TdfF4hoQrL",
        "colab_type": "text"
      },
      "source": [
        "# Putting this together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIkeiy6MoP97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataset_reader(file_path_pattern = \"train/*.csv\", n_readers = 5, \n",
        "                   n_read_threads = None,\n",
        "                   n_parse_threads = 5,\n",
        "                   shuffle_buffer_size = 5000,\n",
        "                   batch_size = 32):\n",
        "  file_path_dataset = tf.data.Dataset.list_files(file_path_pattern)\n",
        "  dataset = file_path_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length = n_readers, num_parallel_calls = n_read_threads\n",
        "  )\n",
        "  dataset = dataset.map(process_raw_input, num_parallel_calls=n_parse_threads)\n",
        "  dataset = dataset.shuffle(shuffle_buffer_size).repeat(2)\n",
        "  return dataset.batch(batch_size).prefetch(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0B8JhgAI2zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = dataset_reader()\n",
        "for ex in train_set.take(5):\n",
        "  print(ex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c9umKmqRbUi",
        "colab_type": "text"
      },
      "source": [
        "We'll also need to sort this for validation and test set, which we'll do by splitting the test set up into 15k examples for validation and 10k examples for test set. We'll do this a bit later, but we will have to put something in place to shuffle the ratings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOrALvjOh0hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "TEST_DIRS = [\"aclImdb/test/neg/\", \"aclImdb/test/pos\"]\n",
        "test_file_list = create_file_list(TEST_DIRS)\n",
        "random.shuffle(test_file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeyS-HiKh15t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "def dataset_creator(file_name, dir_name = \"train\"):\n",
        "  id, rating = os.path.splitext(os.path.basename(file_name))[0].split(\"_\")\n",
        "  review = open(file_name).read()\n",
        "  output =  {'id':int(id), \n",
        "             'rating':int(rating), \n",
        "             'review':review}\n",
        "  output_file = pd.DataFrame(output, index = [0])\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)\n",
        "  output_file.to_csv('{}/{}.csv'.format(dir_name,id+\"_\"+rating))\n",
        "\n",
        "def create_val_test(dirs = TEST_DIRS):\n",
        "  test_file_list = create_file_list(TEST_DIRS)\n",
        "  random.shuffle(test_file_list)\n",
        "  validation_files = test_file_list[:15000]\n",
        "  test_files = test_file_list[15000:]\n",
        "  for f in validation_files:\n",
        "    dataset_creator(f, \"val\")\n",
        "  for f in test_files:\n",
        "    dataset_creator(f, \"test\")\n",
        "  return \"COMPLETE!\"\n",
        "\n",
        "create_val_test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHqbuCwexmnl",
        "colab_type": "text"
      },
      "source": [
        "We can now use our already created functions for creating tf datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-otOupGuyBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset = dataset_reader(file_path_pattern=\"val/*.csv\")\n",
        "test_dataset = dataset_reader(file_path_pattern=\"test/*.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7sIG-LpuwJh",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess the text\n",
        "\n",
        "Now we need to do some standard text preprocessing. \n",
        "\n",
        "1. Do we want to use the entire review, or just the first/last 200 words?\n",
        "2. Convert everything to lower case\n",
        "3. Use some regulatr expressions to sort some of the irritating characters out\n",
        "4. Split each string into individual words.\n",
        "5. Remove punctuation?\n",
        "6. Remove stop words? (Need to check what the more standard approach to this is now?\n",
        "7. Do we want to do any lemmatisation?\n",
        "8. Depending on the techniques that we'd like to use, we may want to use a rought PoS tag?\n",
        "\n",
        "We can use the textVectorisation that comes with Keras to see how well this works. We always always have the option of creating our own embedding layer too. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLmcfd5buv6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "max_features = 3000\n",
        "vectorise_layer = TextVectorization(\n",
        "    max_tokens = max_features,\n",
        "    output_mode = 'int'\n",
        ")\n",
        "vectorise_layer.adapt(train_set.batch(32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4XmsJVHveKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtWlZU3j2LjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}