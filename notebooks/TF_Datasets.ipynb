{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-Datasets",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMznA+gTKzd/NiaEkurSGGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joepeskett/tree-pixels/blob/master/notebooks/TF_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x0vf4NOIxcp",
        "colab_type": "text"
      },
      "source": [
        "# Using TF Datasets\n",
        "\n",
        "There are some really good walkthroughs of how this can be achieved in the [tensorflow data api documentation](https://www.tensorflow.org/guide/data#consuming_text_data). \n",
        "\n",
        "Get the dataset from stanford"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMfwk10ItlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xvf aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzJ7Mak-I3IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "#dataset = tfds.load(\"imdb_reviews\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrTJM3uI3F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to create a similar dataset to the one found in the the tfds package. \n",
        "# How we would we efficiently load these files to create a useful dataset\n",
        "\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLN9IUkWYtaL",
        "colab_type": "text"
      },
      "source": [
        "Notes about this data set - the rating for each movie is in the filename, as is the unique ID of the particular review. \n",
        "\n",
        "A good first step would be to have a setup to have a unique ID, text and the rating. \n",
        "\n",
        "TODO:\n",
        "\n",
        "* List the files within the positive and negative training directories. \n",
        "* Load these into an appropriate training data set\n",
        "* Ensure each piece is text is labelled correctly\n",
        "* Shuffle these dataset\n",
        "* Create an appropriate representation for the text data, likely using the TextVectorisation layer within Keras\n",
        "* Add an embedding layer\n",
        "* Train a model \n",
        "* Try to optimise the pipeline that we've prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hkbKNVtdxD-",
        "colab_type": "text"
      },
      "source": [
        "# First: reformat the dataset.\n",
        "\n",
        "This is so we can make use of the TextLinesDataset ability in the `tf.data` API. \n",
        "\n",
        "Rather than having the rating in the filename, we'll change the txt files to individual "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "besmt-PCI3Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "DIRS = [\"aclImdb/train/pos/\", \"aclImdb/train/neg/\"]\n",
        "def create_file_list(dirs = DIRS):\n",
        "  files = []\n",
        "  for d in dirs:\n",
        "    for f in os.listdir(d):\n",
        "      files.append(os.path.join(d, f))\n",
        "  return files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD-RiqtlfC25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the ratings\n",
        "ratings = []\n",
        "file_names = create_file_list(DIRS)\n",
        "for f in file_names:\n",
        "  id, rating = os.path.splitext(os.path.basename(f))[0].split(\"_\")\n",
        "  ratings.append(int(rating))\n",
        "import numpy as np\n",
        "rate_array = np.array(ratings)\n",
        "np.histogram(rate_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytA7zlnnI28_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple function to grab the ID and rating from the file name\n",
        "# We want to return tf.constants\n",
        "import pandas as pd\n",
        "def dataset_creator(file_name):\n",
        "  id, rating = os.path.splitext(os.path.basename(file_name))[0].split(\"_\")\n",
        "  review = open(file_name).read()\n",
        "  output =  {'id':int(id), \n",
        "             'rating':int(rating), \n",
        "             'review':review}\n",
        "  output_file = pd.DataFrame(output, index = [0])\n",
        "  if not os.path.exists('train'):\n",
        "    os.mkdir('train')\n",
        "  output_file.to_csv('train/{}.csv'.format(id))\n",
        "\n",
        "def create_files(dirs = DIRS):\n",
        "  files = create_file_list(dirs)\n",
        "  for f in files:\n",
        "    dataset_creator(f)\n",
        "  return \"DONE!\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMhJOVk-dSmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_files(DIRS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLnYD3zPgLIp",
        "colab_type": "text"
      },
      "source": [
        "Now, we can start looking at how we'd use the tensorflow api.\n",
        "The key reason to do this is to mimic a setup where we can't git all our training data into memory or we want to start training on a distributed system. \n",
        "\n",
        "In this setup, it's not that easy to use the filename in the initial dataset creation, which is why we've done some initial preprocessing in the above section. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9riud4ZI23C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Now we want to create a data set for each individual file\n",
        "file_path_dataset = tf.data.Dataset.list_files(\"train/*.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmOmJys5vnFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_readers = 5\n",
        "dataset = file_path_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length = n_readers\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lvOnMAzjLhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#What have we got so far?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eck8gpzDi-_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in dataset.take(5):\n",
        "  print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbJAL5cRjnRE",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "At this point we need a preprocessing function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJH7JHMRlThT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(line):\n",
        "  #defs = #The default values o\n",
        "  record_defaults = [[1],[1],[1],tf.constant([], dtype=tf.string)]\n",
        "  fields = tf.io.decode_csv(line, record_defaults=record_defaults)\n",
        "  X = tf.stack(fields[3])\n",
        "  y = tf.stack([2])\n",
        "  return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVcftg2_mqsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in dataset.take(5):\n",
        "  print(preprocess(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7TdfF4hoQrL",
        "colab_type": "text"
      },
      "source": [
        "And now we need to do some shuffling around. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIkeiy6MoP97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0B8JhgAI2zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IDs = []\n",
        "reviews = [] \n",
        "ratings = []\n",
        "for filename in files:\n",
        "  obs = dataset_creator(filename)\n",
        "  IDs.append(obs['id'])\n",
        "  ratings.append(obs['rating'])\n",
        "  reviews.append(obs['review'])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((tf.constant(IDs), tf.constant(ratings), tf.constant(reviews)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOrALvjOh0hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(example_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeyS-HiKh15t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}